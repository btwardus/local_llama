# What is this?

simple UI for testing local code/chat models as well as open interpreter

poetry for dependency mangment

- GGUF, new quantization format for CPU
- GPTQ, quantization for GPU

# Models to use, top performing code generation models:
- 73.8% pass@1 HumanEval
https://huggingface.co/Phind/Phind-CodeLlama-34B-v2
https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-GGUF
https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v2-GPTQ

- 73.2% pass@1 HumanEval
https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0
https://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GGUF
https://huggingface.co/TheBloke/WizardCoder-Python-34B-V1.0-GPTQ


# top model model comparison
https://github.com/emrgnt-cmplxty/zero-shot-replication

https://paperswithcode.com/sota/code-generation-on-humaneval
gpt4 reflection 

# resources
- https://github.com/turboderp/exllama
- https://github.com/turboderp/exllamav2
- https://github.com/noahshinn024/reflexion
- https://www.reddit.com/r/LocalLLaMA/comments/14fqfdu/how_to_use_langchain_with_exllama/
- https://github.com/KillianLucas/open-interpreter
- https://github.com/abetlen/llama-cpp-python

# Wizard coder benchmarks
![Alt text](/resources/image.png)


# Open Interpreter setup
https://github.com/KillianLucas/open-interpreter/blob/main/docs/GPU.md

